import readline from "readline";
import puppeteer from "puppeteer";
import fs from "fs";
import path from "path";
import { URL } from "url";

const visited = new Set();
const outputFolder = "./mirror";

if (!fs.existsSync(outputFolder)) fs.mkdirSync(outputFolder);

/**
 * URL ì •ê·œí™” (#fragment ì œê±°)
 */
function normalizeUrl(fileUrl) {
  return fileUrl.split("#")[0];
}

/**
 * íŒŒì¼ ì €ìž¥ (ì›ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ê·¸ëŒ€ë¡œ)
 */
async function saveFile(fileUrl, buffer) {
  try {
    const cleanUrl = normalizeUrl(fileUrl);
    const urlObj = new URL(cleanUrl);

    // ì›ë³¸ ê²½ë¡œ ê·¸ëŒ€ë¡œ mirror ë°‘ì— ì €ìž¥
    let filePath = path.join(outputFolder, urlObj.pathname);

    // ë””ë ‰í† ë¦¬ URLì´ë©´ index.htmlë¡œ ì €ìž¥
    if (cleanUrl.endsWith("/")) {
      filePath = path.join(filePath, "index.html");
    }

    // ë””ë ‰í† ë¦¬ ìƒì„±
    const dir = path.dirname(filePath);
    if (!fs.existsSync(dir)) {
      fs.mkdirSync(dir, { recursive: true });
    }

    fs.writeFileSync(filePath, buffer);
    console.log(`âœ… ì €ìž¥ë¨: ${filePath}`);
  } catch (err) {
    console.error(`âŒ ì €ìž¥ ì‹¤íŒ¨: ${fileUrl}`, err.message);
  }
}

/**
 * ê°œë³„ íŽ˜ì´ì§€ í¬ë¡¤ë§
 */
async function crawlPage(browser, url, baseDomain, depth, maxDepth) {
  const cleanUrl = normalizeUrl(url);
  if (visited.has(cleanUrl) || depth > maxDepth) return;
  visited.add(cleanUrl);

  try {
    const page = await browser.newPage();

    // ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ì§€Â·ì•„ì´ì½˜Â·í°íŠ¸ í¬í•¨)
    page.on("response", async (response) => {
      try {
        const req = response.request();
        const resUrl = normalizeUrl(req.url());

        if (
          /\.(css|js|png|jpg|jpeg|gif|svg|ico|webp|woff2?|ttf|eot|html?)$/i.test(resUrl) ||
          resUrl.endsWith("/")
        ) {
          const buffer = await response.buffer();
          await saveFile(resUrl, buffer);
        }
      } catch (err) {
        console.error("âŒ ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", err.message);
      }
    });

    await page.goto(cleanUrl, { waitUntil: "networkidle2", timeout: 60000 });

    // HTML ì €ìž¥
    const html = await page.content();
    await saveFile(cleanUrl, Buffer.from(html));

    // ë‚´ë¶€ ë§í¬ ì¶”ì¶œ
    const links = await page.$$eval("a", as => as.map(a => a.href));
    const internalLinks = links.filter(l => {
      try {
        return new URL(l).hostname === baseDomain;
      } catch {
        return false;
      }
    });

    console.log(`ðŸ”— ë°œê²¬ëœ ë‚´ë¶€ ë§í¬ ${internalLinks.length}ê°œ (depth=${depth})`);

    // ìž¬ê·€ íƒìƒ‰
    for (const link of internalLinks) {
      await crawlPage(browser, link, baseDomain, depth + 1, maxDepth);
    }

    await page.close();
  } catch (err) {
    console.error(`âŒ íŽ˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ (${cleanUrl}):`, err.message);
  }
}

/**
 * ì‚¬ì´íŠ¸ ì „ì²´ í¬ë¡¤ë§
 */
async function crawlSite(startUrl, maxDepth = 2) {
  const browser = await puppeteer.launch();
  const baseDomain = new URL(startUrl).hostname;

  await crawlPage(browser, startUrl, baseDomain, 0, maxDepth);

  await browser.close();
}

/**
 * ì‹¤í–‰ë¶€: URL ìž…ë ¥ë°›ê¸°
 */
const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

rl.question("í¬ë¡¤ë§í•  URL ìž…ë ¥: ", (inputUrl) => {
  crawlSite(inputUrl, 2); // depth=2
  rl.close();
});
